{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "import functools\n",
    "import itertools\n",
    "from flatten_dict import flatten, unflatten\n",
    "\n",
    "import torch\n",
    "#from utils.fasttext import text_vectorizer\n",
    "\n",
    "#LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        # Evenly divide the data across the bsz batches.\n",
    "        return ids.view(batch_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameter_grid(parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates all parameter combinations from a dict of parameters like: \n",
    "    {\n",
    "        \"input_length\": 300,\n",
    "        \"layer_parameters\": {\n",
    "            \"conv\": {\"in_channels\": 300, \"out_channels\": [300, 200, 100], \"kernel_size\": [4,5]},\n",
    "            \"maxpool\": {\"kernel_size\": [4,5]},\n",
    "            \"fc\": {\"out_features\": 27, \"bias\": True},\n",
    "            \"dropout\": {\"p\": [0.5, 0.75, 0.9]},\n",
    "        },\n",
    "        \"lr\": [0.01, 0.001, 0.0001],\n",
    "        \"batch_size\": [64, 128, 256]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def wrap_value_to_list(value):\n",
    "        if hasattr(value, \"__iter__\"): \n",
    "            return value \n",
    "        else: \n",
    "            return [value]\n",
    "\n",
    "    def combine_values_with_keys(values, keys):\n",
    "        return {key_value[0]: key_value[1] for key_value in zip(keys, values)}\n",
    "\n",
    "    flattened_dict = flatten(parameters, reducer=\"path\")\n",
    "    flattened_dict = {key: wrap_value_to_list(value) for key, value in flattened_dict.items()}\n",
    "    \n",
    "    create_dict = functools.partial(combine_values_with_keys, keys=flattened_dict.keys())\n",
    "    unflattener = functools.partial(unflatten, splitter=\"path\")\n",
    "    \n",
    "    parameter_combinations = map(create_dict, itertools.product(*flattened_dict.values()))\n",
    "\n",
    "    return list(map(unflattener, parameter_combinations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

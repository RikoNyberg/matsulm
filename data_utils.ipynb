{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "    \n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "    def get_data(self, path, batch_size):\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words: \n",
        "                    self.dictionary.add_word(word)  \n",
        "        \n",
        "        # Tokenize the file content\n",
        "        ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "        num_batches = ids.size(0) // batch_size\n",
        "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "        ids = ids[:num_batches*batch_size]\n",
        "        # Evenly divide the data across the bsz batches.\n",
        "        return ids.view(batch_size, -1)\n",
        "\n",
        "    def batchify(self, data, batch_size, args):\n",
        "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "        nbatch = data.size(0) // batch_size\n",
        "        # Trim off any extra elements that wouldn't fit (remainders, same as data[:num_batches*batch_size]). \n",
        "        data = data.narrow(0, 0, nbatch * batch_size)\n",
        "        # Evenly divide the data across the batch_size batches. (explanation to contiguous() https://stackoverflow.com/a/52229694/9004294)\n",
        "        data = data.view(batch_size, -1).t().contiguous()\n",
        "\n",
        "        if args.cuda:\n",
        "            data = data.cuda()\n",
        "        return data"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some part of the code was referenced from below.\n",
    "# https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from data_utils import Dictionary, Corpus, create_parameter_grid\n",
    "from flatten_dict import flatten, unflatten\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 2\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002\n",
    "log_interval = 100\n",
    "clip_norm = 0.5\n",
    "\n",
    "# Load \"Penn Treebank\" dataset\n",
    "#corpus = Corpus()\n",
    "#ids = corpus.get_data('data/penn/train.txt', batch_size)\n",
    "#vocab_size = len(corpus.dictionary)\n",
    "#num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=False,\n",
    "        ):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, dropout=dropout num_layers=num_layers, batch_first=True,) #bidirectional=bidirectional)\n",
    "        lstm_output_size = hidden_size #if not bidirectional else hidden_size * 2\n",
    "        self.linear = nn.Linear(lstm_output_size, vocab_size)\n",
    "        \n",
    "        for name, param in self.lstm.named_parameters(): # https://discuss.pytorch.org/t/initializing-parameters-of-a-multi-layer-lstm/5791\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal(param)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Dropout vectors\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "####################################################################################\n",
    "# TRAIN\n",
    "####################################################################################\n",
    "def train_lstm_model(train_data, valid_data, test_data, params, max_epochs=50, verbose_logging=True):\n",
    "\n",
    "    def train_model():\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            print('#'*10, f'Epoch [{epoch+1}/{params[\"num_epochs\"]}]', '#'*10)\n",
    "            \n",
    "            # learning rate decay\n",
    "            if params.get('lr_decay') and params.get('lr_decay') =! 1:\n",
    "                new_lr = params['lr'] * (params['lr_decay'] ** max(epoch + 1 - params['lr_decay_start'], 0.0))\n",
    "                print('Learning rate: {:.4f}'.format(learning_rate))\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = new_lr\n",
    "            \n",
    "            train_epoch_loss = predict(train_data, train=True)\n",
    "            valid_epoch_loss = predict(valid_data, train=False)\n",
    "\n",
    "            if verbose_logging:\n",
    "                print('-'*10, f'End of Epoch {epoch+1}', '-'*10)\n",
    "                print('Train Loss: {:.4f}, Train Perplexity: {:5.2f}'\n",
    "                    .format(train_epoch_loss, np.exp(train_epoch_loss)))\n",
    "                print('Valid Loss: {:.4f}, Valid Perplexity: {:5.2f}'\n",
    "                    .format(valid_epoch_loss, np.exp(valid_epoch_loss)))\n",
    "                print('-'*40)\n",
    "        \n",
    "        test_epoch_loss = predict(test_data, train=False)            \n",
    "        print('-'*10, f'Test set results', '-'*10)\n",
    "        print('Test Loss: {:.4f}, Test Perplexity: {:5.2f}'\n",
    "                .format(test_epoch_loss, np.exp(test_epoch_loss)))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def predict(data, train=False):\n",
    "        if train:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        # Set initial hidden and cell states\n",
    "        states = (\n",
    "            torch.zeros(\n",
    "                params['model']['num_layers'],# * (2 if params['model']['bidirectional'] else 1), \n",
    "                params['batch_size'], \n",
    "                params['model']['hidden_size'],\n",
    "            ).to(device),\n",
    "            torch.zeros(\n",
    "                params['model']['num_layers'],# * (2 if params['model']['bidirectional'] else 1), \n",
    "                params['batch_size'], \n",
    "                params['model']['hidden_size'],\n",
    "            ).to(device)\n",
    "        )\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(0, data.size(1) - params['seq_length'], params['seq_length']):\n",
    "            # Get mini-batch inputs and targets\n",
    "            inputs = data[:, i:i+params['seq_length']].to(device)\n",
    "            targets = data[:, (i+1):(i+1)+params['seq_length']].to(device)\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            # https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/4\n",
    "            states = detach(states)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, states = model(inputs, states)\n",
    "            loss = criterion(outputs, targets.reshape(-1)) # in here the targets.reshape(-1) is the same as the .t() transpose in the batchify\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if train:\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), params['clip_norm'])\n",
    "                optimizer.step()\n",
    "\n",
    "            step = (i+1) // params['seq_length']\n",
    "            if step % params['log_interval'] == 0 and i != 0 and verbose_logging:\n",
    "                loss_mean = sum(losses[-params['log_interval']:]) / params['log_interval']\n",
    "                print('Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                    .format(step, data.size(1) // params['seq_length'], loss_mean, np.exp(loss_mean)))\n",
    "        \n",
    "        loss_mean = sum(losses) / len(losses)\n",
    "        return loss_mean\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if params['cuda'] and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(params[\"model\"])\n",
    "    \n",
    "    if params.get('seed'):\n",
    "        torch.manual_seed(params['seed'])\n",
    "    model = RNNLM(**params[\"model\"]).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if params['optimizer'] == 'sgd':\n",
    "        self.optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    if params['optimizer'] == 'adam':\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "        \n",
    "    return model, train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_lstm(data_path):\n",
    "    parameters = {\n",
    "        'model': {\n",
    "            'embed_size': 128,\n",
    "            'hidden_size': [1024, 300],\n",
    "            'num_layers': 1,\n",
    "            'dropout': 0.5\n",
    "            #'bidirectional': [False, True],\n",
    "        },\n",
    "        'num_epochs': 2,\n",
    "        'batch_size': 20,\n",
    "        'seq_length': 30,\n",
    "        'log_interval': 300,\n",
    "        'clip_norm': 0.5,\n",
    "        'lr': 0.002,\n",
    "        'cuda': True,\n",
    "        'seed': 313,\n",
    "        ##\n",
    "        \n",
    "        'lr_decay_start':    6,\n",
    "        'lr_decay':    0.8,\n",
    "        'optimizer':    'sgd',\n",
    "        'weight_decay': 0, # weight decay applied to all weights (0 = no decay)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Load \"Penn Treebank\" dataset\n",
    "    corpus = Corpus()\n",
    "    train_data = corpus.get_data(os.path.join(data_path, 'train.txt'), parameters['batch_size'])\n",
    "    valid_data = corpus.get_data(os.path.join(data_path, 'valid.txt'), parameters['batch_size'])\n",
    "    test_data = corpus.get_data(os.path.join(data_path, 'test.txt'), parameters['batch_size'])\n",
    "\n",
    "    parameters['model']['vocab_size'] = len(corpus.dictionary)\n",
    "    print('vocab_size: ', parameters['model']['vocab_size'])\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    all_parameters = create_parameter_grid(parameters)\n",
    "    \n",
    "    for index, params in enumerate(all_parameters):\n",
    "        LOGGER.info(\"\\nTuning %s/%s\", index+1, len(all_parameters))\n",
    "        LOGGER.info(\"Parameters: %s\", json.dumps(params, indent=4, default=str))\n",
    "        start = time.time()\n",
    "        _, results = train_lstm_model(\n",
    "            train_data,\n",
    "            valid_data,\n",
    "            test_data,\n",
    "            params=params,\n",
    "            max_epochs=50,\n",
    "            verbose_logging=True\n",
    "            )\n",
    "        \n",
    "        # LOGGER.info(\"Results: %s\", json.dumps(results, indent=4, default=str))\n",
    "        LOGGER.info(\"Training took: %ss\", time.time()-start)\n",
    "        all_results.append({\"parameters\": params, \"results\": results})\n",
    "        \n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Tuning 1/4\n",
      "INFO:__main__:Parameters: {\n",
      "    \"model\": {\n",
      "        \"embed_size\": 128,\n",
      "        \"hidden_size\": 1024,\n",
      "        \"num_layers\": 1,\n",
      "        \"bidirectional\": false,\n",
      "        \"vocab_size\": 10000\n",
      "    },\n",
      "    \"num_epochs\": 2,\n",
      "    \"batch_size\": 20,\n",
      "    \"seq_length\": 30,\n",
      "    \"log_interval\": 300,\n",
      "    \"clip_norm\": 0.5,\n",
      "    \"lr\": 0.002,\n",
      "    \"num_samples\": 1000,\n",
      "    \"cuda\": true\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  10000\n",
      "{'embed_size': 128, 'hidden_size': 1024, 'num_layers': 1, 'bidirectional': False, 'vocab_size': 10000}\n",
      "########## Epoch [1/2] ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/04/nybergr1/unix/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/u/04/nybergr1/unix/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step[300/1549], Loss: 6.0453, Perplexity: 422.14\n",
      "Step[600/1549], Loss: 5.4975, Perplexity: 244.08\n",
      "Step[900/1549], Loss: 5.2349, Perplexity: 187.71\n",
      "Step[1200/1549], Loss: 5.1129, Perplexity: 166.15\n",
      "Step[1500/1549], Loss: 4.8727, Perplexity: 130.67\n",
      "---------- End of Epoch 1 ----------\n",
      "Train Loss: 5.3476, Train Perplexity: 210.10\n",
      "Valid Loss: 4.9520, Valid Perplexity: 141.46\n",
      "----------------------------------------\n",
      "########## Epoch [2/2] ##########\n",
      "Step[300/1549], Loss: 4.6833, Perplexity: 108.13\n",
      "Step[600/1549], Loss: 4.4808, Perplexity: 88.31\n",
      "Step[900/1549], Loss: 4.3361, Perplexity: 76.41\n",
      "Step[1200/1549], Loss: 4.3098, Perplexity: 74.42\n",
      "Step[1500/1549], Loss: 4.0955, Perplexity: 60.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training took: 65.67201852798462s\n",
      "INFO:__main__:\n",
      "Tuning 2/4\n",
      "INFO:__main__:Parameters: {\n",
      "    \"model\": {\n",
      "        \"embed_size\": 128,\n",
      "        \"hidden_size\": 1024,\n",
      "        \"num_layers\": 1,\n",
      "        \"bidirectional\": true,\n",
      "        \"vocab_size\": 10000\n",
      "    },\n",
      "    \"num_epochs\": 2,\n",
      "    \"batch_size\": 20,\n",
      "    \"seq_length\": 30,\n",
      "    \"log_interval\": 300,\n",
      "    \"clip_norm\": 0.5,\n",
      "    \"lr\": 0.002,\n",
      "    \"num_samples\": 1000,\n",
      "    \"cuda\": true\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- End of Epoch 2 ----------\n",
      "Train Loss: 4.3807, Train Perplexity: 79.90\n",
      "Valid Loss: 4.9013, Valid Perplexity: 134.46\n",
      "----------------------------------------\n",
      "{'embed_size': 128, 'hidden_size': 1024, 'num_layers': 1, 'bidirectional': True, 'vocab_size': 10000}\n",
      "########## Epoch [1/2] ##########\n",
      "Step[300/1549], Loss: 2.6058, Perplexity: 13.54\n",
      "Step[600/1549], Loss: 0.7607, Perplexity:  2.14\n",
      "Step[900/1549], Loss: 0.5299, Perplexity:  1.70\n",
      "Step[1200/1549], Loss: 0.4674, Perplexity:  1.60\n",
      "Step[1500/1549], Loss: 0.4321, Perplexity:  1.54\n",
      "---------- End of Epoch 1 ----------\n",
      "Train Loss: 0.9485, Train Perplexity:  2.58\n",
      "Valid Loss: 0.4525, Valid Perplexity:  1.57\n",
      "----------------------------------------\n",
      "########## Epoch [2/2] ##########\n",
      "Step[300/1549], Loss: 0.3567, Perplexity:  1.43\n",
      "Step[600/1549], Loss: 0.3086, Perplexity:  1.36\n",
      "Step[900/1549], Loss: 0.2917, Perplexity:  1.34\n",
      "Step[1200/1549], Loss: 0.2768, Perplexity:  1.32\n",
      "Step[1500/1549], Loss: 0.2623, Perplexity:  1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training took: 117.92710971832275s\n",
      "INFO:__main__:\n",
      "Tuning 3/4\n",
      "INFO:__main__:Parameters: {\n",
      "    \"model\": {\n",
      "        \"embed_size\": 128,\n",
      "        \"hidden_size\": 300,\n",
      "        \"num_layers\": 1,\n",
      "        \"bidirectional\": false,\n",
      "        \"vocab_size\": 10000\n",
      "    },\n",
      "    \"num_epochs\": 2,\n",
      "    \"batch_size\": 20,\n",
      "    \"seq_length\": 30,\n",
      "    \"log_interval\": 300,\n",
      "    \"clip_norm\": 0.5,\n",
      "    \"lr\": 0.002,\n",
      "    \"num_samples\": 1000,\n",
      "    \"cuda\": true\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- End of Epoch 2 ----------\n",
      "Train Loss: 0.2986, Train Perplexity:  1.35\n",
      "Valid Loss: 0.4588, Valid Perplexity:  1.58\n",
      "----------------------------------------\n",
      "{'embed_size': 128, 'hidden_size': 300, 'num_layers': 1, 'bidirectional': False, 'vocab_size': 10000}\n",
      "########## Epoch [1/2] ##########\n",
      "Step[300/1549], Loss: 6.2727, Perplexity: 529.88\n",
      "Step[600/1549], Loss: 5.6858, Perplexity: 294.66\n",
      "Step[900/1549], Loss: 5.4095, Perplexity: 223.51\n",
      "Step[1200/1549], Loss: 5.2757, Perplexity: 195.52\n",
      "Step[1500/1549], Loss: 5.0567, Perplexity: 157.07\n",
      "---------- End of Epoch 1 ----------\n",
      "Train Loss: 5.5339, Train Perplexity: 253.12\n",
      "Valid Loss: 5.1215, Valid Perplexity: 167.59\n",
      "----------------------------------------\n",
      "########## Epoch [2/2] ##########\n",
      "Step[300/1549], Loss: 4.9450, Perplexity: 140.47\n",
      "Step[600/1549], Loss: 4.8075, Perplexity: 122.42\n",
      "Step[900/1549], Loss: 4.7044, Perplexity: 110.43\n",
      "Step[1200/1549], Loss: 4.6851, Perplexity: 108.32\n",
      "Step[1500/1549], Loss: 4.5033, Perplexity: 90.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training took: 24.296768188476562s\n",
      "INFO:__main__:\n",
      "Tuning 4/4\n",
      "INFO:__main__:Parameters: {\n",
      "    \"model\": {\n",
      "        \"embed_size\": 128,\n",
      "        \"hidden_size\": 300,\n",
      "        \"num_layers\": 1,\n",
      "        \"bidirectional\": true,\n",
      "        \"vocab_size\": 10000\n",
      "    },\n",
      "    \"num_epochs\": 2,\n",
      "    \"batch_size\": 20,\n",
      "    \"seq_length\": 30,\n",
      "    \"log_interval\": 300,\n",
      "    \"clip_norm\": 0.5,\n",
      "    \"lr\": 0.002,\n",
      "    \"num_samples\": 1000,\n",
      "    \"cuda\": true\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- End of Epoch 2 ----------\n",
      "Train Loss: 4.7304, Train Perplexity: 113.35\n",
      "Valid Loss: 4.9490, Valid Perplexity: 141.04\n",
      "----------------------------------------\n",
      "{'embed_size': 128, 'hidden_size': 300, 'num_layers': 1, 'bidirectional': True, 'vocab_size': 10000}\n",
      "########## Epoch [1/2] ##########\n",
      "Step[300/1549], Loss: 3.7231, Perplexity: 41.39\n",
      "Step[600/1549], Loss: 1.4057, Perplexity:  4.08\n",
      "Step[900/1549], Loss: 0.8308, Perplexity:  2.30\n",
      "Step[1200/1549], Loss: 0.6251, Perplexity:  1.87\n",
      "Step[1500/1549], Loss: 0.5080, Perplexity:  1.66\n",
      "---------- End of Epoch 1 ----------\n",
      "Train Loss: 1.3950, Train Perplexity:  4.04\n",
      "Valid Loss: 0.4965, Valid Perplexity:  1.64\n",
      "----------------------------------------\n",
      "########## Epoch [2/2] ##########\n",
      "Step[300/1549], Loss: 0.3788, Perplexity:  1.46\n",
      "Step[600/1549], Loss: 0.3277, Perplexity:  1.39\n",
      "Step[900/1549], Loss: 0.3091, Perplexity:  1.36\n",
      "Step[1200/1549], Loss: 0.2984, Perplexity:  1.35\n",
      "Step[1500/1549], Loss: 0.2805, Perplexity:  1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training took: 37.24978232383728s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- End of Epoch 2 ----------\n",
      "Train Loss: 0.3183, Train Perplexity:  1.37\n",
      "Valid Loss: 0.4446, Valid Perplexity:  1.56\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'parameters': {'model': {'embed_size': 128,\n",
       "    'hidden_size': 1024,\n",
       "    'num_layers': 1,\n",
       "    'bidirectional': False,\n",
       "    'vocab_size': 10000},\n",
       "   'num_epochs': 2,\n",
       "   'batch_size': 20,\n",
       "   'seq_length': 30,\n",
       "   'log_interval': 300,\n",
       "   'clip_norm': 0.5,\n",
       "   'lr': 0.002,\n",
       "   'num_samples': 1000,\n",
       "   'cuda': True},\n",
       "  'results': True},\n",
       " {'parameters': {'model': {'embed_size': 128,\n",
       "    'hidden_size': 1024,\n",
       "    'num_layers': 1,\n",
       "    'bidirectional': True,\n",
       "    'vocab_size': 10000},\n",
       "   'num_epochs': 2,\n",
       "   'batch_size': 20,\n",
       "   'seq_length': 30,\n",
       "   'log_interval': 300,\n",
       "   'clip_norm': 0.5,\n",
       "   'lr': 0.002,\n",
       "   'num_samples': 1000,\n",
       "   'cuda': True},\n",
       "  'results': True},\n",
       " {'parameters': {'model': {'embed_size': 128,\n",
       "    'hidden_size': 300,\n",
       "    'num_layers': 1,\n",
       "    'bidirectional': False,\n",
       "    'vocab_size': 10000},\n",
       "   'num_epochs': 2,\n",
       "   'batch_size': 20,\n",
       "   'seq_length': 30,\n",
       "   'log_interval': 300,\n",
       "   'clip_norm': 0.5,\n",
       "   'lr': 0.002,\n",
       "   'num_samples': 1000,\n",
       "   'cuda': True},\n",
       "  'results': True},\n",
       " {'parameters': {'model': {'embed_size': 128,\n",
       "    'hidden_size': 300,\n",
       "    'num_layers': 1,\n",
       "    'bidirectional': True,\n",
       "    'vocab_size': 10000},\n",
       "   'num_epochs': 2,\n",
       "   'batch_size': 20,\n",
       "   'seq_length': 30,\n",
       "   'log_interval': 300,\n",
       "   'clip_norm': 0.5,\n",
       "   'lr': 0.002,\n",
       "   'num_samples': 1000,\n",
       "   'cuda': True},\n",
       "  'results': True}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_tune_lstm('data/penn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some part of the code was referenced from below.\n",
    "# https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from data_utils import Dictionary, Corpus, create_parameter_grid\n",
    "from flatten_dict import flatten, unflatten\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Penn Treebank\" dataset\n",
    "#corpus = Corpus()\n",
    "#ids = corpus.get_data('data/penn/train.txt', batch_size)\n",
    "#vocab_size = len(corpus.dictionary)\n",
    "#num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=False,\n",
    "            init_weight=None,\n",
    "            init_bias=0,\n",
    "            forget_bias=1,\n",
    "        ):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #bidirectional=bidirectional)\n",
    "        lstm_output_size = hidden_size #if not bidirectional else hidden_size * 2\n",
    "        self.linear = nn.Linear(lstm_output_size, vocab_size)\n",
    "        \n",
    "        # Initializing weights/bias\n",
    "        init_weight = 1.0/np.sqrt(hidden_size) if not init_weight else init_weight\n",
    "        for name, param in self.lstm.named_parameters(): # https://discuss.pytorch.org/t/initializing-parameters-of-a-multi-layer-lstm/5791\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, init_bias)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.uniform_(param, -init_weight, init_weight)\n",
    "        \n",
    "        # Setting Forget Gate bias\n",
    "        for names in self.lstm._all_weights:\n",
    "            for name in filter(lambda n: \"bias\" in n,  names):\n",
    "                bias = getattr(self.lstm, name)\n",
    "                n = bias.size(0)\n",
    "                start, end = n//4, n//2\n",
    "                bias.data[start:end].fill_(1.)\n",
    "                \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Dropout vectors\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        # Softmax\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out, (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "####################################################################################\n",
    "# TRAIN\n",
    "####################################################################################\n",
    "def train_lstm_model(train_data, valid_data, test_data, params, max_epochs=50, verbose_logging=True):\n",
    "\n",
    "    def train_model():\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            print('#'*10, f'Epoch [{epoch+1}/{params[\"num_epochs\"]}]', '#'*10)\n",
    "            \n",
    "            # learning rate decay\n",
    "            if params.get('lr_decay') and params.get('lr_decay') != 1:\n",
    "                new_lr = params['lr'] * (params['lr_decay'] ** max(epoch + 1 - params['lr_decay_start'], 0.0))\n",
    "                print('Learning rate: {:.4f}'.format(new_lr))\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = new_lr\n",
    "            \n",
    "            train_epoch_loss = predict(train_data, train=True)\n",
    "            valid_epoch_loss = predict(valid_data, train=False)\n",
    "\n",
    "            if verbose_logging:\n",
    "                print('-'*10, f'End of Epoch {epoch+1}', '-'*10)\n",
    "                print('Train Loss: {:.4f}, Train Perplexity: {:5.2f}'\n",
    "                    .format(train_epoch_loss, np.exp(train_epoch_loss)))\n",
    "                print('Valid Loss: {:.4f}, Valid Perplexity: {:5.2f}'\n",
    "                    .format(valid_epoch_loss, np.exp(valid_epoch_loss)))\n",
    "                print('-'*40)\n",
    "        \n",
    "        test_epoch_loss = predict(test_data, train=False)            \n",
    "        print('-'*10, f'Test set results', '-'*10)\n",
    "        print('Test Loss: {:.4f}, Test Perplexity: {:5.2f}'\n",
    "                .format(test_epoch_loss, np.exp(test_epoch_loss)))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def predict(data, train=False):\n",
    "        if train:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        # Set initial hidden and cell states\n",
    "        states = (\n",
    "            torch.zeros(\n",
    "                params['model']['num_layers'],# * (2 if params['model']['bidirectional'] else 1), \n",
    "                params['batch_size'], \n",
    "                params['model']['hidden_size'],\n",
    "            ).to(device),\n",
    "            torch.zeros(\n",
    "                params['model']['num_layers'],# * (2 if params['model']['bidirectional'] else 1), \n",
    "                params['batch_size'], \n",
    "                params['model']['hidden_size'],\n",
    "            ).to(device)\n",
    "        )\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(0, data.size(1) - params['seq_length'], params['seq_length']):\n",
    "            # Get mini-batch inputs and targets\n",
    "            inputs = data[:, i:i+params['seq_length']].to(device)\n",
    "            targets = data[:, (i+1):(i+1)+params['seq_length']].to(device)\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            # https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/4\n",
    "            states = detach(states)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, states = model(inputs, states)\n",
    "            loss = criterion(outputs, targets.reshape(-1)) # in here the targets.reshape(-1) is the same as the .t() transpose in the batchify\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if train:\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), params['clip_norm'])\n",
    "                optimizer.step()\n",
    "\n",
    "            step = (i+1) // params['seq_length']\n",
    "            if step % params['log_interval'] == 0 and i != 0 and verbose_logging:\n",
    "                loss_mean = sum(losses[-params['log_interval']:]) / params['log_interval']\n",
    "                print('Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                    .format(step, data.size(1) // params['seq_length'], loss_mean, np.exp(loss_mean)))\n",
    "        \n",
    "        loss_mean = sum(losses) / len(losses)\n",
    "        return loss_mean\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if params['cuda'] and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(params[\"model\"])\n",
    "    \n",
    "    if params.get('seed'):\n",
    "        torch.manual_seed(params['seed'])\n",
    "    model = RNNLM(**params[\"model\"]).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if params['optimizer'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError('Missing optimizer parameter')\n",
    "        \n",
    "    return model, train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_lstm(data_path):\n",
    "    parameters = {\n",
    "        'model': {\n",
    "            'embed_size': 128,\n",
    "            'hidden_size': [1024],\n",
    "            'num_layers': 1,\n",
    "            'dropout': 0.5,\n",
    "            #'bidirectional': [False, True],\n",
    "            'init_weight': 0.05, # (float): the weights of the model will be randomly initialized, with a uniform distribution and values between -init_scale and init_scale\n",
    "            'init_bias': 0,\n",
    "            'forget_bias': 0,\n",
    "        },\n",
    "        'num_epochs': 2,\n",
    "        'batch_size': 20,\n",
    "        'seq_length': 30,\n",
    "        'log_interval': 300,\n",
    "        'clip_norm': 0.5,\n",
    "        'lr': 0.002,\n",
    "        'cuda': True,\n",
    "        'seed': 313,\n",
    "        'lr_decay_start': 6,\n",
    "        'lr_decay': 0.8,\n",
    "        'optimizer': ['adam'],\n",
    "        'weight_decay': 0, # weight decay applied to all weights (0 = no decay)\n",
    "    }\n",
    "    \n",
    "    parameters = {\n",
    "        'model': {\n",
    "            'num_layers': 1,\n",
    "            'embed_size': 512,\n",
    "            'hidden_size': 512,\n",
    "            'init_weight': 0.05,\n",
    "            'dropout': 0.5,\n",
    "            'init_bias': 0,\n",
    "            'forget_bias': 0,\n",
    "        },\n",
    "        'log_interval': 300,\n",
    "        'cuda': [True],\n",
    "        'seed': 313,\n",
    "        'weight_decay': 0,\n",
    "        'optimizer': [\"sgd\"],\n",
    "        'lr_decay_start': 6,\n",
    "        'lr': 1,\n",
    "        'seq_length': 35,\n",
    "        'batch_size': 20,\n",
    "        'lr_decay': 0.8,\n",
    "        'clip_norm': 5,\n",
    "        'num_epochs': 39,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Load \"Penn Treebank\" dataset\n",
    "    corpus = Corpus()\n",
    "    train_data = corpus.get_data(os.path.join(data_path, 'train.txt'), parameters['batch_size'])\n",
    "    valid_data = corpus.get_data(os.path.join(data_path, 'valid.txt'), parameters['batch_size'])\n",
    "    test_data = corpus.get_data(os.path.join(data_path, 'test.txt'), parameters['batch_size'])\n",
    "\n",
    "    parameters['model']['vocab_size'] = len(corpus.dictionary)\n",
    "    print('vocab_size: ', parameters['model']['vocab_size'])\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    all_parameters = create_parameter_grid(parameters)\n",
    "    \n",
    "    for index, params in enumerate(all_parameters):\n",
    "        LOGGER.info(\"\\nTuning %s/%s\", index+1, len(all_parameters))\n",
    "        LOGGER.info(\"Parameters: %s\", json.dumps(params, indent=4, default=str))\n",
    "        start = time.time()\n",
    "        _, results = train_lstm_model(\n",
    "            train_data,\n",
    "            valid_data,\n",
    "            test_data,\n",
    "            params=params,\n",
    "            max_epochs=50,\n",
    "            verbose_logging=True\n",
    "            )\n",
    "        \n",
    "        # LOGGER.info(\"Results: %s\", json.dumps(results, indent=4, default=str))\n",
    "        LOGGER.info(\"Training took: %ss\", time.time()-start)\n",
    "        all_results.append({\"parameters\": params, \"results\": results})\n",
    "        \n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameter_tune_lstm('data/penn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

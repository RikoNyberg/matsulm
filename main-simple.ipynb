{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some part of the code was referenced from below.\n",
    "# https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from data_utils import Dictionary, Corpus, create_parameter_grid\n",
    "from flatten_dict import flatten, unflatten\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Penn Treebank\" dataset\n",
    "#corpus = Corpus()\n",
    "#ids = corpus.get_data('data/penn/train.txt', batch_size)\n",
    "#vocab_size = len(corpus.dictionary)\n",
    "#num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            bidirectional=False,\n",
    "            init_weight=None,\n",
    "            init_bias=0,\n",
    "            forget_bias=1,\n",
    "        ):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #bidirectional=bidirectional)\n",
    "        lstm_output_size = hidden_size #if not bidirectional else hidden_size * 2\n",
    "        self.linear = nn.Linear(lstm_output_size, vocab_size)\n",
    "        \n",
    "        # Initializing weights/bias\n",
    "        init_weight = 1.0/np.sqrt(hidden_size) if not init_weight else init_weight\n",
    "        for name, param in self.lstm.named_parameters(): # https://discuss.pytorch.org/t/initializing-parameters-of-a-multi-layer-lstm/5791\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, init_bias)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.uniform_(param, -init_weight, init_weight)\n",
    "        \n",
    "        # Setting Forget Gate bias\n",
    "        for names in self.lstm._all_weights:\n",
    "            for name in filter(lambda n: \"bias\" in n,  names):\n",
    "                bias = getattr(self.lstm, name)\n",
    "                n = bias.size(0)\n",
    "                start, end = n//4, n//2\n",
    "                bias.data[start:end].fill_(1.)\n",
    "                \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Dropout vectors\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, (h, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "####################################################################################\n",
    "# TRAIN\n",
    "####################################################################################\n",
    "def train_lstm_model(train_data, valid_data, test_data, params, max_epochs=50, verbose_logging=True):\n",
    "\n",
    "    def train_model():\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            print('#'*10, f'Epoch [{epoch+1}/{params[\"num_epochs\"]}]', '#'*10)\n",
    "            \n",
    "            # learning rate decay\n",
    "            if params.get('lr_decay') and params.get('lr_decay') != 1:\n",
    "                new_lr = params['lr'] * (params['lr_decay'] ** max(epoch + 1 - params['lr_decay_start'], 0.0))\n",
    "                print('Learning rate: {:.4f}'.format(new_lr))\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = new_lr\n",
    "            \n",
    "            train_epoch_loss = predict(train_data, train=True)\n",
    "            valid_epoch_loss = predict(valid_data, train=False)\n",
    "\n",
    "            if verbose_logging:\n",
    "                print('-'*10, f'End of Epoch {epoch+1}', '-'*10)\n",
    "                print('Train Loss: {:.4f}, Train Perplexity: {:5.2f}'\n",
    "                    .format(train_epoch_loss, np.exp(train_epoch_loss)))\n",
    "                print('Valid Loss: {:.4f}, Valid Perplexity: {:5.2f}'\n",
    "                    .format(valid_epoch_loss, np.exp(valid_epoch_loss)))\n",
    "                print('-'*40)\n",
    "        \n",
    "        test_epoch_loss = predict(test_data, train=False)            \n",
    "        print('-'*10, f'Test set results', '-'*10)\n",
    "        print('Test Loss: {:.4f}, Test Perplexity: {:5.2f}'\n",
    "                .format(test_epoch_loss, np.exp(test_epoch_loss)))\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def predict(data, train=False):\n",
    "        if train:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        # Set initial hidden and cell states\n",
    "        states = (\n",
    "            torch.zeros(\n",
    "                params['model']['num_layers'],# * (2 if params['model']['bidirectional'] else 1), \n",
    "                params['batch_size'], \n",
    "                params['model']['hidden_size'],\n",
    "            ).to(device),\n",
    "            torch.zeros(\n",
    "                params['model']['num_layers'],# * (2 if params['model']['bidirectional'] else 1), \n",
    "                params['batch_size'], \n",
    "                params['model']['hidden_size'],\n",
    "            ).to(device)\n",
    "        )\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(0, data.size(1) - params['seq_length'], params['seq_length']):\n",
    "            # Get mini-batch inputs and targets\n",
    "            inputs = data[:, i:i+params['seq_length']].to(device)\n",
    "            targets = data[:, (i+1):(i+1)+params['seq_length']].to(device)\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            # https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/4\n",
    "            states = detach(states)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, states = model(inputs, states)\n",
    "            loss = criterion(outputs, targets.reshape(-1)) # in here the targets.reshape(-1) is the same as the .t() transpose in the batchify\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if train:\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(model.parameters(), params['clip_norm'])\n",
    "                optimizer.step()\n",
    "\n",
    "            step = (i+1) // params['seq_length']\n",
    "            if step % params['log_interval'] == 0 and i != 0 and verbose_logging:\n",
    "                loss_mean = sum(losses[-params['log_interval']:]) / params['log_interval']\n",
    "                print('Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                    .format(step, data.size(1) // params['seq_length'], loss_mean, np.exp(loss_mean)))\n",
    "        \n",
    "        loss_mean = sum(losses) / len(losses)\n",
    "        return loss_mean\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if params['cuda'] and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(params[\"model\"])\n",
    "    \n",
    "    if params.get('seed'):\n",
    "        torch.manual_seed(params['seed'])\n",
    "    model = RNNLM(**params[\"model\"]).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if params['optimizer'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    elif params['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError('Missing optimizer parameter')\n",
    "        \n",
    "    return model, train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_lstm(data_path):\n",
    "    parameters = {\n",
    "        'model': {\n",
    "            'num_layers': 1,\n",
    "            'embed_size': 100,\n",
    "            'hidden_size': 256,\n",
    "            'init_weight': 1.0,\n",
    "            'dropout': 0,\n",
    "            'init_bias': 0,\n",
    "            'forget_bias': 0,\n",
    "        },\n",
    "        'log_interval': 300,\n",
    "        'cuda': [True],\n",
    "        'seed': 313,\n",
    "        'weight_decay': 0,\n",
    "        'optimizer': [\"sgd\"],\n",
    "        'num_epochs': 20,\n",
    "        'lr_decay_start': 20,\n",
    "        'lr': 1,\n",
    "        'seq_length': 35,\n",
    "        'batch_size': 20,\n",
    "        'lr_decay': 0.8,\n",
    "        'clip_norm': 5,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Load \"Penn Treebank\" dataset\n",
    "    corpus = Corpus()\n",
    "    train_data = corpus.get_data(os.path.join(data_path, 'train.txt'), parameters['batch_size'])\n",
    "    valid_data = corpus.get_data(os.path.join(data_path, 'valid.txt'), parameters['batch_size'])\n",
    "    test_data = corpus.get_data(os.path.join(data_path, 'test.txt'), parameters['batch_size'])\n",
    "\n",
    "    parameters['model']['vocab_size'] = len(corpus.dictionary)\n",
    "    print('vocab_size: ', parameters['model']['vocab_size'])\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    all_parameters = create_parameter_grid(parameters)\n",
    "    \n",
    "    for index, params in enumerate(all_parameters):\n",
    "        LOGGER.info(\"\\nTuning %s/%s\", index+1, len(all_parameters))\n",
    "        LOGGER.info(\"Parameters: %s\", json.dumps(params, indent=4, default=str))\n",
    "        start = time.time()\n",
    "        _, results = train_lstm_model(\n",
    "            train_data,\n",
    "            valid_data,\n",
    "            test_data,\n",
    "            params=params,\n",
    "            max_epochs=50,\n",
    "            verbose_logging=True\n",
    "            )\n",
    "        \n",
    "        # LOGGER.info(\"Results: %s\", json.dumps(results, indent=4, default=str))\n",
    "        LOGGER.info(\"Training took: %ss\", time.time()-start)\n",
    "        all_results.append({\"parameters\": params, \"results\": results})\n",
    "        \n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Tuning 1/1\n",
      "INFO:__main__:Parameters: {\n",
      "    \"model\": {\n",
      "        \"num_layers\": 1,\n",
      "        \"embed_size\": 100,\n",
      "        \"hidden_size\": 256,\n",
      "        \"init_weight\": 0.0,\n",
      "        \"dropout\": 0,\n",
      "        \"init_bias\": 0,\n",
      "        \"forget_bias\": 0,\n",
      "        \"vocab_size\": 10000\n",
      "    },\n",
      "    \"log_interval\": 300,\n",
      "    \"cuda\": true,\n",
      "    \"seed\": 313,\n",
      "    \"weight_decay\": 0,\n",
      "    \"optimizer\": \"sgd\",\n",
      "    \"num_epochs\": 20,\n",
      "    \"lr_decay_start\": 20,\n",
      "    \"lr\": 1,\n",
      "    \"seq_length\": 35,\n",
      "    \"batch_size\": 20,\n",
      "    \"lr_decay\": 0.8,\n",
      "    \"clip_norm\": 5\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  10000\n",
      "{'num_layers': 1, 'embed_size': 100, 'hidden_size': 256, 'init_weight': 0.0, 'dropout': 0, 'init_bias': 0, 'forget_bias': 0, 'vocab_size': 10000}\n",
      "########## Epoch [1/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 6.6898, Perplexity: 804.14\n",
      "Step[600/1327], Loss: 6.2879, Perplexity: 538.00\n",
      "Step[900/1327], Loss: 6.0824, Perplexity: 438.09\n",
      "Step[1200/1327], Loss: 5.9595, Perplexity: 387.42\n",
      "---------- End of Epoch 1 ----------\n",
      "Train Loss: 6.2222, Train Perplexity: 503.82\n",
      "Valid Loss: 5.9060, Valid Perplexity: 367.25\n",
      "----------------------------------------\n",
      "########## Epoch [2/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.8479, Perplexity: 346.50\n",
      "Step[600/1327], Loss: 5.8036, Perplexity: 331.49\n",
      "Step[900/1327], Loss: 5.6945, Perplexity: 297.24\n",
      "Step[1200/1327], Loss: 5.6324, Perplexity: 279.33\n",
      "---------- End of Epoch 2 ----------\n",
      "Train Loss: 5.7312, Train Perplexity: 308.34\n",
      "Valid Loss: 5.6512, Valid Perplexity: 284.64\n",
      "----------------------------------------\n",
      "########## Epoch [3/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.5879, Perplexity: 267.18\n",
      "Step[600/1327], Loss: 5.5705, Perplexity: 262.56\n",
      "Step[900/1327], Loss: 5.4879, Perplexity: 241.75\n",
      "Step[1200/1327], Loss: 5.4424, Perplexity: 230.99\n",
      "---------- End of Epoch 3 ----------\n",
      "Train Loss: 5.5130, Train Perplexity: 247.90\n",
      "Valid Loss: 5.5014, Valid Perplexity: 245.04\n",
      "----------------------------------------\n",
      "########## Epoch [4/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.4217, Perplexity: 226.26\n",
      "Step[600/1327], Loss: 5.4121, Perplexity: 224.10\n",
      "Step[900/1327], Loss: 5.3444, Perplexity: 209.44\n",
      "Step[1200/1327], Loss: 5.3063, Perplexity: 201.61\n",
      "---------- End of Epoch 4 ----------\n",
      "Train Loss: 5.3642, Train Perplexity: 213.61\n",
      "Valid Loss: 5.4044, Valid Perplexity: 222.39\n",
      "----------------------------------------\n",
      "########## Epoch [5/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.2982, Perplexity: 199.98\n",
      "Step[600/1327], Loss: 5.2912, Perplexity: 198.58\n",
      "Step[900/1327], Loss: 5.2330, Perplexity: 187.35\n",
      "Step[1200/1327], Loss: 5.1995, Perplexity: 181.18\n",
      "---------- End of Epoch 5 ----------\n",
      "Train Loss: 5.2499, Train Perplexity: 190.55\n",
      "Valid Loss: 5.3322, Valid Perplexity: 206.89\n",
      "----------------------------------------\n",
      "########## Epoch [6/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.1996, Perplexity: 181.20\n",
      "Step[600/1327], Loss: 5.1932, Perplexity: 180.04\n",
      "Step[900/1327], Loss: 5.1414, Perplexity: 170.96\n",
      "Step[1200/1327], Loss: 5.1122, Perplexity: 166.04\n",
      "---------- End of Epoch 6 ----------\n",
      "Train Loss: 5.1570, Train Perplexity: 173.64\n",
      "Valid Loss: 5.2765, Valid Perplexity: 195.69\n",
      "----------------------------------------\n",
      "########## Epoch [7/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.1171, Perplexity: 166.85\n",
      "Step[600/1327], Loss: 5.1100, Perplexity: 165.67\n",
      "Step[900/1327], Loss: 5.0635, Perplexity: 158.14\n",
      "Step[1200/1327], Loss: 5.0375, Perplexity: 154.09\n",
      "---------- End of Epoch 7 ----------\n",
      "Train Loss: 5.0781, Train Perplexity: 160.47\n",
      "Valid Loss: 5.2322, Valid Perplexity: 187.20\n",
      "----------------------------------------\n",
      "########## Epoch [8/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 5.0458, Perplexity: 155.38\n",
      "Step[600/1327], Loss: 5.0384, Perplexity: 154.23\n",
      "Step[900/1327], Loss: 4.9958, Perplexity: 147.79\n",
      "Step[1200/1327], Loss: 4.9725, Perplexity: 144.39\n",
      "---------- End of Epoch 8 ----------\n",
      "Train Loss: 5.0097, Train Perplexity: 149.87\n",
      "Valid Loss: 5.1964, Valid Perplexity: 180.62\n",
      "----------------------------------------\n",
      "########## Epoch [9/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.9834, Perplexity: 145.97\n",
      "Step[600/1327], Loss: 4.9753, Perplexity: 144.79\n",
      "Step[900/1327], Loss: 4.9359, Perplexity: 139.20\n",
      "Step[1200/1327], Loss: 4.9146, Perplexity: 136.26\n",
      "---------- End of Epoch 9 ----------\n",
      "Train Loss: 4.9493, Train Perplexity: 141.07\n",
      "Valid Loss: 5.1677, Valid Perplexity: 175.52\n",
      "----------------------------------------\n",
      "########## Epoch [10/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.9316, Perplexity: 138.61\n",
      "Step[600/1327], Loss: 4.9187, Perplexity: 136.83\n",
      "Step[900/1327], Loss: 4.8823, Perplexity: 131.93\n",
      "Step[1200/1327], Loss: 4.8623, Perplexity: 129.32\n",
      "---------- End of Epoch 10 ----------\n",
      "Train Loss: 4.8960, Train Perplexity: 133.76\n",
      "Valid Loss: 5.1435, Valid Perplexity: 171.31\n",
      "----------------------------------------\n",
      "########## Epoch [11/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.8771, Perplexity: 131.25\n",
      "Step[600/1327], Loss: 4.8673, Perplexity: 129.97\n",
      "Step[900/1327], Loss: 4.8336, Perplexity: 125.67\n",
      "Step[1200/1327], Loss: 4.8146, Perplexity: 123.29\n",
      "---------- End of Epoch 11 ----------\n",
      "Train Loss: 4.8458, Train Perplexity: 127.20\n",
      "Valid Loss: 5.1230, Valid Perplexity: 167.84\n",
      "----------------------------------------\n",
      "########## Epoch [12/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.8308, Perplexity: 125.31\n",
      "Step[600/1327], Loss: 4.8204, Perplexity: 124.02\n",
      "Step[900/1327], Loss: 4.7891, Perplexity: 120.19\n",
      "Step[1200/1327], Loss: 4.7708, Perplexity: 118.01\n",
      "---------- End of Epoch 12 ----------\n",
      "Train Loss: 4.8006, Train Perplexity: 121.59\n",
      "Valid Loss: 5.1058, Valid Perplexity: 164.98\n",
      "----------------------------------------\n",
      "########## Epoch [13/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.7878, Perplexity: 120.04\n",
      "Step[600/1327], Loss: 4.7771, Perplexity: 118.76\n",
      "Step[900/1327], Loss: 4.7478, Perplexity: 115.33\n",
      "Step[1200/1327], Loss: 4.7302, Perplexity: 113.32\n",
      "---------- End of Epoch 13 ----------\n",
      "Train Loss: 4.7588, Train Perplexity: 116.61\n",
      "Valid Loss: 5.0919, Valid Perplexity: 162.69\n",
      "----------------------------------------\n",
      "########## Epoch [14/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.7480, Perplexity: 115.35\n",
      "Step[600/1327], Loss: 4.7369, Perplexity: 114.08\n",
      "Step[900/1327], Loss: 4.7092, Perplexity: 110.96\n",
      "Step[1200/1327], Loss: 4.6924, Perplexity: 109.11\n",
      "---------- End of Epoch 14 ----------\n",
      "Train Loss: 4.7199, Train Perplexity: 112.16\n",
      "Valid Loss: 5.0796, Valid Perplexity: 160.71\n",
      "----------------------------------------\n",
      "########## Epoch [15/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.7107, Perplexity: 111.13\n",
      "Step[600/1327], Loss: 4.6995, Perplexity: 109.90\n",
      "Step[900/1327], Loss: 4.6729, Perplexity: 107.00\n",
      "Step[1200/1327], Loss: 4.6566, Perplexity: 105.28\n",
      "---------- End of Epoch 15 ----------\n",
      "Train Loss: 4.6834, Train Perplexity: 108.14\n",
      "Valid Loss: 5.0695, Valid Perplexity: 159.09\n",
      "----------------------------------------\n",
      "########## Epoch [16/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.6757, Perplexity: 107.30\n",
      "Step[600/1327], Loss: 4.6644, Perplexity: 106.10\n",
      "Step[900/1327], Loss: 4.6386, Perplexity: 103.40\n",
      "Step[1200/1327], Loss: 4.6231, Perplexity: 101.81\n",
      "---------- End of Epoch 16 ----------\n",
      "Train Loss: 4.6491, Train Perplexity: 104.49\n",
      "Valid Loss: 5.0602, Valid Perplexity: 157.62\n",
      "----------------------------------------\n",
      "########## Epoch [17/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.6424, Perplexity: 103.80\n",
      "Step[600/1327], Loss: 4.6309, Perplexity: 102.61\n",
      "Step[900/1327], Loss: 4.6062, Perplexity: 100.10\n",
      "Step[1200/1327], Loss: 4.5911, Perplexity: 98.61\n",
      "---------- End of Epoch 17 ----------\n",
      "Train Loss: 4.6164, Train Perplexity: 101.13\n",
      "Valid Loss: 5.0527, Valid Perplexity: 156.44\n",
      "----------------------------------------\n",
      "########## Epoch [18/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.6108, Perplexity: 100.57\n",
      "Step[600/1327], Loss: 4.5992, Perplexity: 99.41\n",
      "Step[900/1327], Loss: 4.5752, Perplexity: 97.05\n",
      "Step[1200/1327], Loss: 4.5609, Perplexity: 95.67\n",
      "---------- End of Epoch 18 ----------\n",
      "Train Loss: 4.5854, Train Perplexity: 98.04\n",
      "Valid Loss: 5.0446, Valid Perplexity: 155.19\n",
      "----------------------------------------\n",
      "########## Epoch [19/20] ##########\n",
      "Learning rate: 1.0000\n",
      "Step[300/1327], Loss: 4.5806, Perplexity: 97.57\n",
      "Step[600/1327], Loss: 4.5690, Perplexity: 96.45\n",
      "Step[900/1327], Loss: 4.5456, Perplexity: 94.22\n",
      "Step[1200/1327], Loss: 4.5319, Perplexity: 92.93\n",
      "---------- End of Epoch 19 ----------\n",
      "Train Loss: 4.5558, Train Perplexity: 95.18\n",
      "Valid Loss: 5.0388, Valid Perplexity: 154.29\n",
      "----------------------------------------\n",
      "########## Epoch [20/20] ##########\n",
      "Learning rate: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step[300/1327], Loss: 4.5518, Perplexity: 94.80\n",
      "Step[600/1327], Loss: 4.5400, Perplexity: 93.69\n",
      "Step[900/1327], Loss: 4.5173, Perplexity: 91.59\n",
      "Step[1200/1327], Loss: 4.5042, Perplexity: 90.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training took: 220.072092294693s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- End of Epoch 20 ----------\n",
      "Train Loss: 4.5274, Train Perplexity: 92.52\n",
      "Valid Loss: 5.0342, Valid Perplexity: 153.57\n",
      "----------------------------------------\n",
      "---------- Test set results ----------\n",
      "Test Loss: 5.5433, Test Perplexity: 255.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'parameters': {'model': {'num_layers': 1,\n",
       "    'embed_size': 100,\n",
       "    'hidden_size': 256,\n",
       "    'init_weight': 0.0,\n",
       "    'dropout': 0,\n",
       "    'init_bias': 0,\n",
       "    'forget_bias': 0,\n",
       "    'vocab_size': 10000},\n",
       "   'log_interval': 300,\n",
       "   'cuda': True,\n",
       "   'seed': 313,\n",
       "   'weight_decay': 0,\n",
       "   'optimizer': 'sgd',\n",
       "   'num_epochs': 20,\n",
       "   'lr_decay_start': 20,\n",
       "   'lr': 1,\n",
       "   'seq_length': 35,\n",
       "   'batch_size': 20,\n",
       "   'lr_decay': 0.8,\n",
       "   'clip_norm': 5},\n",
       "  'results': True}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_tune_lstm('data/penn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
